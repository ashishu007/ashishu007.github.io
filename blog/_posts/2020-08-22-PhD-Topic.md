---
layout: post
title: "Why I Choose Data-to-Text Generation for my PhD"
date: 2020-08-22
desc: "Looking into my research topic in PhD"
keywords: "NLG, Data-to-Text, NLP"
categories: [NLP, NLG, D2T]
tags: [NLP, D2T, PhD]
blog: [Blog]
excerpt_separator: <!--more-->
images: 
    - url: /assets/phd/nlp.png
    - url: /assets/phd/seq2seq.png
icon: icon-html
---

I started my PhD with a goal of learning NLP in depth. I had recently completed my internship on document analysis and was very excited to know what lies next. From the intial reading I got to know how much there is to work in NLP. Unfortunately, I cannot work on everything just in my PhD. So, after wandering a little more, I finally decided to work on Dat-to-Text Generation. In this bog, I'll try to write why I choose this field.

<!--more-->

**Table of Contents**

- [Motivation](#motivation)
- [NLP Background](#nlp-background)
	* [Natural Language Understanding - NLU](#natural-language-understanding---nlu)
	* [Natural Language Generation - NLG](#natural-language-generation---nlg)
- [More on D2T NLG](#more-on-d2t-nlg)
	* [Current Benchmarks in NLG](#current-benchmarks-in-nlg)

<small><i><a href='http://ecotrust-canada.github.io/markdown-toc/'>Table of contents generated with markdown-toc</a></i></small>

## Motivation
The evolution of web has led to an enormous growth in the amounts of data and information available for organisations to employ in their everyday operations. Different business processes such as healthcare, medical analysis, compliance management require a lot of textual documents to be processed to produce effective results. Applying novel Natural Language Processing (NLP) techniques to these document analysis tasks can reduce human effort and error, thus reducing the overall cost.

For the last couple of years, developments in Deep Learning (DL) systems have produced efficient results in different AI tasks. Most of these systems are being deployed into the real-world applications. As DL methods are completely data-driven, they also reduce the requirement of domain expertise. However, they do need huge amount of labelled data to train themselves. Getting that labelled data is very expensive for real-word application as they require subject matter experts to manually label data points.

The potential of DL systems with the mentioned drawback makes a compelling point of developing algorithms that can help these systems learn from fewer (or none) labelled samples. In this PhD, I will explore the possibility of developing such novel algorithms capable of helping DL systems learn better in the filed of NLP.

## NLP Background
NLP is a huge research field. It consists of several tasks from understanding the natural language in problems like Named Entity Recognition (NER) and Text Classification to generating the natural language in problems like Machine Translation (MT) and Document Summarisation. Broadly, NLP can be classified into two different subtasks as shown in figure below: 

![NLP](/assets/phd/nlp.png)
<div style="text-align: center;"><b>NLP Taxonomy</b></div>

### Natural Language Understanding - NLU
NLU aims towards making AI models understand the human language in order to achieve some goal. Generally, this goal is to assign a class label to either whole textual document or the sequence of tokens from a text document. It consists of tasks such as: sentiment analysis, reading comprehension, relation extraction or natural language inference. 

### Natural Language Generation - NLG
NLG, on the other hand, aims towards making AI models generate a sequential output in form of natural language for given some input (linguistic or non-linguistic). Based on the type of the input given to the system, NLG can be further classified into two different categories: 

1. **Text-to-Text Generation (T2T NLG)**: where the input to the systems is in the linguistic form. For example, machine translation, where we provide input in one natural language and the system generates the same sentence/document into different language as output.
2. **Data-to-Text Generation (D2T NLG)**: on the other hand, takes non-linguistic data such as: tables or knowledge graphs, as its input and generates natural language summary of that structured input as the output. For example, summarising a football match based on the scorecard of a game.

In my PhD, I am interested in exploring the possibility of using DL systems in solving Data-to-Text Generation problem with minimal labelled data

## More on D2T NLG
The reason for choosing this topic is because NLG hasn’t received a lot of attention from the DL community (if we’re to leave the developments in Neural Machine Translation). NLG, as compared to its counterpart, NLU, is still very under-developed. Part of the reason maybe not having many direct applications that impact the real-world. On the other hand, NLU, has several applications in the real-world and thus has received lots of attention from the academia and industry. 

But, that said, NLG (D2T in particular) have several real-world applications where DL systems can improve the performance. NLG is widely used in finance and weather reporting domains [[1]](#myfootnote1), although they are limited to rule-based systems. There are several applications in healthcare as well [[2]](#myfootnote2), which has started to come out, probably due to some of the recent developments in other area of NLP. I have written about D2T NLG: its subtasks, public datasets, and some applications in details in a [blog here](https://panditu2015.github.io/Data-to-Text-Generation/).


### Current Benchmarks in NLG
As Sequence-to-Sequence (seq2seq) models are benchmarks in several NLG tasks, D2T NLG is also no exception. Almost all the current DL benchmarks in D2T NLG are some variation of seq2seq models. As described earlier, D2T NLG being under-developed in terms of Deep Learning, there are several rule-based systems that still outperform the heavily trained DL systems.

As I am interested in exploring the DL version of the solution to this problem, I’ll lay out some of the key points that are important features of these seq2seq models.

1. **Encoders**: first, the non-linguistic input data is encoded into a context vector using a neural network. This network is usually an RNN (or LSTM, to be specific) or a Transformer. Sometimes, Graph Neural Nets are also used, depending on the nature of the input (e.g., in case of RDF triples to text problem).
2. **Decoders**: after the encoder has encoded the input into a context (thought as mentioned in figure above) vector, a decoder neural network is used to generate the output sequence, one token at a time. These decoder neural nets are also some variation of an RNN or Transformer.
3. **Other updates**: there have been several updates to these seq2seq models, to name a few important ones: 
	- **Attention Mechanism**: to solve the bottleneck problem that is created by encoding a long sequence into one single context vector; 
	- **Copy Mechanism**: to mitigate the problem of out-of-vocabulary words by copying the word from input sequence if it is not present in the generation vocabulary, and; 
	- **Coverage Mechanism**: to stop the models from generating repetitive sentences or phrases in the output.

A simple seq2seq model is shown in figure below:

![S2S](/assets/phd/seq2seq.png)
<div style="text-align: center;"><b>A Simple seq2seq (Encoder-Decoder) Architecture</b></div>

There’s an open-source community called [NLP-progress](http://nlpprogress.com/) that tracks the developments in the field of NLP in several languages. There I wrote [an article to document the state-of-the-art in D2T NLG](https://nlpprogress.com/english/data_to_text_generation.html).


## References

<a name="myfootnote1">[1]</a> Albert Gatt and Emiel Krahmer.  Survey of the state of the art in natural language generation:  Core tasks, applications and evaluation. Journal of Artificial Intelligence Research, 61:65–170, 2018.

<a name="myfootnote2">[2]</a> Steffen Pauws, Albert Gatt, Emiel Krahmer, and Ehud Reiter. Making effective use of healthcare data usingdata-to-text technology.  In Data Science for Healthcare, pages 119–145. Springer, 2019.